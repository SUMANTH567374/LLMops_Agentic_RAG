# File: 5_agent/hybrid_qa_agent.py

import yaml
import re
from typing import List
from Pipeline.embeddings_2.query_vector_store import query_vector_store
from Pipeline.tools_4.serper_search import SerperSearchAgent
from Pipeline.llm_3.gemini_generate import generate_with_gemini
from logging_config import setup_logger

# Load configuration from config.yaml
with open("config.yaml", "r") as f:
    config = yaml.safe_load(f)

logger = setup_logger("hybrid_agent")

# Extract config values
ADVANCED_SYSTEM_PROMPT = config["prompt"]["system_prompt"]
TOP_K = config["vector_store"]["top_k"]
SERPER_MAX_RESULTS = config["serper"]["max_results"]

# Fallback triggers list
FALLBACK_TRIGGERS = [
    "does not contain",
    "no relevant information",
    "i'm sorry",
    "context is insufficient",
    "i do not know",
    "not enough information",
    "could not find",
    "unable to answer",
    "information is not available",
    "i don't have",
    "i cannot",
    "i'm not sure",
    "as an ai",
    "i was not able to find",
    "this text does not include"
]

def build_prompt(context: str, user_query: str) -> str:
    """
    Builds a complete prompt for the LLM using context and user input.
    """
    return f"""{ADVANCED_SYSTEM_PROMPT}

Context:
{context}

User Question:
{user_query}

Your Answer (strictly based on the context above):
"""

def needs_fallback(answer: str, triggers: List[str]) -> bool:
    """
    Check if the LLM's answer requires fallback to Serper.
    """
    clean_answer = re.sub(r'[^\w\s]', '', answer.lower())
    return any(trigger in clean_answer for trigger in triggers)

def hybrid_qa_agent(user_query: str, top_k: int = TOP_K) -> str:
    """
    Hybrid RAG pipeline with fallback to web search using Serper.

    Args:
        user_query (str): The user‚Äôs question.
        top_k (int): Number of vector results to retrieve.

    Returns:
        str: Answer generated by LLM.
    """
    try:
        logger.info(f"üì© Received user query: '{user_query}'")
        logger.info("üîç Step 1: Querying local vector DB...")

        chunks = query_vector_store(user_query, k=top_k)

        if chunks:
            logger.info(f"‚úÖ Retrieved {len(chunks)} relevant chunks from vector DB.")
            context = "\n".join(chunks)
        else:
            logger.warning("‚ö†Ô∏è No data found in vector DB. Using Serper for web search...")
            agent = SerperSearchAgent()
            web_results = agent.search(user_query, max_results=SERPER_MAX_RESULTS)

            if not web_results:
                logger.error("‚ùå No relevant content found from Serper search.")
                return "I'm sorry, I could not find any relevant information for your question."

            context = "\n".join(web_results)
            logger.info(f"üåê Collected {len(web_results)} web snippets via Serper.")

        final_prompt = build_prompt(context, user_query)
        logger.info("ü§ñ Sending prompt to AI LLM...")
        answer = generate_with_gemini(final_prompt)
        logger.info("‚úÖ AI response received.")

        # If answer is too vague, fallback to Serper search
        if needs_fallback(answer, FALLBACK_TRIGGERS):
            logger.warning("‚ö†Ô∏è LLM could not answer from context. Retrying with Serper fallback...")

            agent = SerperSearchAgent()
            web_results = agent.search(user_query, max_results=SERPER_MAX_RESULTS)

            if not web_results:
                logger.error("‚ùå No relevant content found from Serper fallback.")
                return "I'm sorry, I could not find any relevant information for your question."

            context = "\n".join(web_results)
            final_prompt = build_prompt(context, user_query)

            logger.info("üîÅ Re-sending prompt to LLM using Serper context...")
            answer = generate_with_gemini(final_prompt)
            logger.info("‚úÖ AI response from Serper fallback received.")

        return answer

    except Exception as e:
        logger.exception("üö® Exception in hybrid_qa_agent:", exc_info=e)
        return "An error occurred while generating your response."
